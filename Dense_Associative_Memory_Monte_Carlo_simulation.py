import jax
from functools import partial
jax.config.update("jax_enable_x64", True)

def sigmoid(x):
    '''
    Numerically stable sigmoid(x).
    Used as a helper function.
    '''
    c = jax.numpy.maximum(x, 0)
    m = jax.numpy.exp(x - c)
    return m / (m + jax.numpy.exp(-c))

class Model():
    '''
    Create a dense Hopfield model and sample from its Gibbs distribution using a Monte-Carlo simulation.
    
    Attributes
    ----------
    name (str):
        Write "gardner" for the Hamiltonian studied by Gardner and "krotov" for the Hamiltonian studied by Krotov.
    p (int):
        Interaction order of the dense Hopfield model.
    N (int):
        Number of entries in the patterns memorized and generated by the dense Hopfield network.
    M (int):
        Number of patterns memorized by the dense Hopfield network.
    L (int):
        Number of patterns generated by the dense Hopfield network.
    spin_batch_size (int):
        Number of spins updated per Monte-Carlo step.
    key (int):
        Seed of random number generation.
    
    Methods
    -------
    init_spins:
        Initialize memorized and generated patterns at random.
        Memorized patterns can be supplied as an argument if they are known.
    gardner_energy_diff:
        Compute the energy lost by flipping a batch of spins under Gardner's Hamiltonian.
        This method accepts only p = 2, p = 3 and p = 4.
    krotov_energy_diff:
        Compute the energy lost by flipping a batch of spins under Krotov's Hamiltonian.
    stochastic_rule:
        Accept spin flips stochastically according to Glauber dynamics.
    deterministic_rule:
        Accept spin flips if and only if the energy difference is positive. Used at zero temperature.
    update_spins:
        Suggest spin flips and decide whether to accept or reject the result.
    generate_spins:
        The Monte-Carlo simulation. It runs update_spins multiple times.
    '''
    def __init__(self, name, p, N, M, L, spin_batch_size, key):
        self.name = name
        
        self.p = p
        
        self.N = N
        self.M = M
        self.L = L
        
        self.spin_batch_size = spin_batch_size
        
        if self.name != "gardner" and self.name != "krotov":
            raise ValueError("Model name not recognized. Supported options are 'gardner' and 'krotov'.")
        
        if not 1 <= self.spin_batch_size <= self.N:
            raise ValueError("spin_batch_size must be between 1 and N.")
        
        if self.name == "krotov" and self.spin_batch_size == self.N:
            raise RuntimeWarning("It is not recommended to use synchronous updates for the krotov model due to slow convergence.")
        
        self.L_range = jax.numpy.arange(L)
        
        self.spin_values = jax.numpy.array([-1, 1], dtype = "float32")
        
        self.key_init, self.key_update = jax.random.split(key)
        
        return
    
    @partial(jax.jit, static_argnums = (0, 1))
    def init_spins(self, init_overlap = 0, ori_spins = None):
        '''
        Initialize memorized and generated patterns at random.
        Memorized patterns can be supplied as an argument if they are already known.
        
        Inputs
        ------
        init_overlap (float):
            Initial overlap between the memorized and generated patterns.
        ori_spins (float array or None):
            Patterns memorized by the network if they are known.
        
        Outputs
        -------
        ori_spins (float array):
            Patterns memorized by the network.
        gen_spins (float array):
            Initial values for the patterns generated by the networks.
            gen_spins is meant to be updated by the method called generate_spins.
        '''
        key_ori, key_gen_1, key_gen_2 = jax.random.split(self.key_init, num = 3)
        if ori_spins is None:
            ori_spins = jax.random.choice(key_ori, self.spin_values, (self.M, self.N))
        
        if -1 <= init_overlap <= 1:
            gen_spins = jax.random.choice(key_gen_1, jax.numpy.array([-1, 1], dtype = "float32"), (self.N, self.L),
                                          p = jax.numpy.array([(1 - init_overlap)/2, (1 + init_overlap)/2]))
        else:
            raise ValueError("init_overlap must be a number between -1 and 1")
        
        gen_spins = gen_spins * ori_spins[: self.L].T
        
        return ori_spins, gen_spins
    
    @partial(jax.jit, static_argnums = 0)
    def gardner_energy_diff(self, overlaps, ori_spins, gen_spins):
        '''
        Compute the energy lost by flipping a batch of spins under Gardner's Hamiltonian.
        This method accepts only p = 2, p = 3 and p = 4.
        
        Inputs
        ------
        overlaps (float array):
            Overlaps of gen_spins with ori_spins.
        ori_spins (float array):
            Batch of spins memorized by the network.
        gen_spins (float array):
            Batch of spins that are being generated by the networks.
        
        Outputs
        -------
        dE (float array):
            Energy lost by flipping a batch of spins under Gardner's Hamiltonian.
        '''
        if self.p == 2:
            h = -2 * jax.lax.dot(ori_spins.T, overlaps, precision = jax.lax.Precision.HIGH)
            
        elif self.p == 3:
            h = -3 * jax.lax.dot(ori_spins.T, overlaps**2 - 1/self.N, precision = jax.lax.Precision.HIGH)
            
        elif self.p == 4:
            h = -4 * jax.lax.dot(ori_spins.T, (overlaps**2 - 3/self.N)*overlaps, precision = jax.lax.Precision.HIGH)
            
        else:
            raise ValueError("Only p = 2, p = 3 and p = 4 can be used when model == 'gardner'.")
        
        return 2 * h * gen_spins
    
    @partial(jax.jit, static_argnums = 0)
    def krotov_energy_diff(self, overlaps, ori_spins, gen_spins):
        '''
        Compute the energy lost by flipping a batch of spins under Krotov's Hamiltonian.
        
        Inputs
        ------
        overlaps (float array):
            Overlaps of gen_spins with ori_spins.
        ori_spins (float array):
            Batch of spins memorized by the network.
        gen_spins (float array):
            Batch of spins that are being generated by the networks.
        
        Outputs
        -------
        dE (float array):
            Energy lost by flipping a batch of spins under Krotov's Hamiltonian.
        '''
        # (M, L) = (M, batch_size) @ (batch_size, L)
        shifts = -2 * jax.numpy.matmul(ori_spins, gen_spins, precision = jax.lax.Precision.HIGHEST)/self.N
        
        return self.N * jax.numpy.sum(jax.numpy.where(jax.numpy.abs(overlaps) > jax.numpy.abs(shifts),
                                                      overlaps**self.p * jax.numpy.expm1(self.p * jax.numpy.log1p(shifts / overlaps)),
                                                      (overlaps + shifts)**self.p - overlaps**self.p), axis = 0, dtype = "float64")
    
    @partial(jax.jit, static_argnums = 0)
    def stochastic_rule(self, beta, dE, key_accept):
        '''
        Accept spin flips stochastically according to Glauber dynamics.
        
        Inputs
        ------
        beta (float):
            Inverse temperature.
        dE (float):
            Energy difference computed using either gardner_energy_diff or krotov_energy_diff.
        key_accept (int):
            Seed of random number generation.
        
        Outputs
        -------
        accept_spins (bool array):
            Whether spin flips are accepted.
        '''
        return jax.random.uniform(key_accept, (dE.shape)) <= sigmoid(beta * dE)
    
    @partial(jax.jit, static_argnums = 0)
    def deterministic_rule(self, beta, dE, key_accept):
        '''
        Accept spin flips deterministically if dE > 0.
        
        Inputs
        ------
        beta (float):
            Inverse temperature. Placeholder to make the signature of this function the same as stochastic_rule.
        dE (float):
            Energy difference computed using either gardner_energy_diff or krotov_energy_diff.
        key_accept (int):
            Seed of random number generation. Placeholder to make the signature of this function the same as stochastic_rule.
        
        Outputs
        -------
        accept_spins (bool array):
            Whether spin flips are accepted.
        '''
        return dE > 0
    
    @partial(jax.jit, static_argnums = 0)
    def update_spins(self, beta, ori_spins, t_cur, args):
        '''
        Suggest spin flips and decide whether to accept or reject the result.
        
        Inputs
        ------
        beta (float):
            Inverse temperature.
        ori_spins (float array):
            Patterns memorized by the network.
        t_cur (int):
            Current step of the Monte-Carlo simulation.
        args (tuple, (gen_spins, key_update)):
            gen_spins (float_array):
                Patterns that are being generated by the network.
            key_update (int):
                Seed of random number generation.
        
        Outputs
        -------
        gen_spins (float_array):
            Patterns that are being generated by the network.
        key_update (int):
            Seed of random number generation.
        '''
        gen_spins, key_update = args
        
        # (M, L) = (M, N) @ (N, L)
        overlaps = jax.lax.dot(ori_spins, gen_spins, precision = jax.lax.Precision.HIGH)/self.N
        
        if self.name == "krotov" and self.spin_batch_size == self.N:
            raise RuntimeWarning("It is not recommended to use synchronous updates for the krotov model due to slow convergence.")
        
        if self.spin_batch_size == self.N:
            key_update, key_accept = jax.random.split(key_update, num = 2)
            i = ...
        
        elif 0 < self.spin_batch_size < self.N:
            key_update, key_i, key_accept = jax.random.split(key_update, num = 3)
            i = jax.random.choice(key_i, self.N, (self.spin_batch_size,), replace = False)
        
        else:
            raise ValueError("spin_batch_size must be between 1 and N.")
        
        if self.name == "gardner":
            dE = self.gardner_energy_diff(overlaps, ori_spins[:, i], gen_spins[i])
        
        elif self.name == "krotov":
            dE = self.krotov_energy_diff(overlaps, ori_spins[:, i], gen_spins[i])
        
        else:
            raise ValueError("Model name not recognized. Supported options are 'gardner' and 'krotov'.")
        
        accept_spins = jax.lax.cond(jax.numpy.isfinite(beta), self.stochastic_rule, self.deterministic_rule, beta, dE, key_accept)
        
        gen_spins = gen_spins.at[i].multiply(1 - 2*accept_spins)
        
        return gen_spins, key_update
    
    @partial(jax.jit, static_argnums = (0, 1))
    def generate_spins(self, t, beta, ori_spins, gen_spins):
        '''
        The Monte-Carlo simulation. It runs update_spins multiple times.
        
        Inputs
        ------
        t (int):
            Number of Monte-Carlo steps.
        beta (float):
            Inverse temperature.
        ori_spins (float array):
            Patterns memorized by the network.
        gen_spins (float array):
            Initial values for the patterns generated by the networks.
        
        Outputs
        -------
        gen_spins (float array):
            Patterns generated by the network.
        
        '''
        gen_spins, key_update = jax.lax.fori_loop(0, t, partial(self.update_spins, beta, ori_spins), (gen_spins, self.key_update))
        
        return gen_spins